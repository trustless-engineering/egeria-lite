{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0axMRQcaFmDc"
   },
   "source": [
    "<!-- Banner Image -->\n",
    "<img src=\"https://github.com/trustless-engineering/egeria-lite/blob/main/docs/egeria-lite.png?raw=true\" width=\"100%\">\n",
    "\n",
    "<!-- Links -->\n",
    "<center>\n",
    "  <a href=\"https://www.trustless.engineering/\" style=\"color: #06b6d4;\">Website</a> ‚Ä¢\n",
    "  <a href=\"https://discord.com/invite/pmbc4NjctV\" style=\"color: #06b6d4;\">Discord</a>\n",
    "</center>\n",
    "\n",
    "# Egeria Lite\n",
    "## A simple, customizable, predictive-forecasting model for Solana defi tokens\n",
    "\n",
    "üìä In this notebook, you will compile, train, and run predictions against your own version of our Egeria token risk model.\n",
    "\n",
    "üé• Don't forget to check out our accompanying video walk-through for a step-by-step guide!\n",
    "\n",
    "üåê We'll build this using Vybe Network API's, but you can augment the model to include any variables that you find personally risky. Then, we'll dive into training our model using the powerful XGBoost algorithm.\n",
    "\n",
    "üîç Throughout this notebook, we'll explore the ins and outs of XGBoost and how it can help us tackle the regression problem at hand.\n",
    "\n",
    "üìù Help us enhance this tutorial even further! Share your feedback and thoughts on our [Discord Server](https://discord.com/invite/pmbc4NjctV) or directly on [X](https://x.com/trustlesseng). Your input is invaluable in making this tutorial the best it can be! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5Hc0M-8IR0r"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Make sure you have all the necessary packages pre-installed. Run the following cell to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2iUFRZmIUaR",
    "outputId": "d51cdd2d-0247-4341-cc2b-769d67094b72"
   },
   "outputs": [],
   "source": [
    "#Installing necessary modules.\n",
    "%pip install -U pandas\n",
    "%pip install -U scikit-learn\n",
    "%pip install -U numpy\n",
    "%pip install -U matplotlib\n",
    "%pip install xgboost==2.0.3\n",
    "%pip install pandas==2.2.1\n",
    "%pip install scikit-learn==1.4.1.post1\n",
    "%pip install joblib==1.3.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the various modules into our working environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daLiBQd7W43v"
   },
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "76Pskf92S-W5"
   },
   "source": [
    "# Let's Build Your Dataset! üìä\n",
    "Before we dive into training our model, we need to create a robust training dataset. This dataset will be in the form of a .json file and will contain essential variables sourced from various Vybe token data APIs.\n",
    "Here's an example of how an entry in your dataset should look:\n",
    "\n",
    "```{\n",
    "    \"address\": \"CxBaBF4XJtn9HDzSiNg2sLq8C34VADKzbk3DNH2Lufug\",\n",
    "    \"decimals\": 9,\n",
    "    \"lastTradeUnixTime\": null,\n",
    "    \"liquidity\": 72.98954780564692,\n",
    "    \"logoURI\": \"https://img.fotofolio.xyz/?url=https%3A%2F%2Fgateway.irys.xyz%2FtLep8ljpMHgiybkmGKTuYlWe5MsJxHuKpYE58d7fzyQ\",\n",
    "    \"mc\": null,\n",
    "    \"name\": \"lola\",\n",
    "    \"symbol\": \"lola\",\n",
    "    \"v24hChangePercent\": null,\n",
    "    \"v24hUSD\": 0,\n",
    "    \"Risk\": \"Danger\",\n",
    "    \"Volatility\": 72.01814851023744,\n",
    "    \"holders_count\": 1\n",
    "}\n",
    "```\n",
    "\n",
    "Remember, the quality of your data directly impacts the performance and accuracy of your model. So, let's strive for excellence in data quality to unlock the full potential of our model! Note that our base dataset has roughly 2000 entries. We recommend at least 1600 entries that have a decent spread of safe and dangerous tokens labeled.üöÄ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bdgsnf8GW2si"
   },
   "source": [
    "# Let's Load Your Training Data! üçΩ\n",
    "Now that we have our dataset ready, it's time to load it into our model. We'll be using this data to train our machine learning model to accurately classify tokens based on their risk levels.\n",
    "\n",
    "Make sure your dataset is in the proper format, containing the essential variables: address, decimals, liquidity, logoURI, name, symbol, v24hChangePercent, v24hUSD, Risk, Volatility, and holders_count.\n",
    "\n",
    "Once we load the data, we'll be ready to embark on our machine learning journey and unlock valuable insights from the cryptocurrency market! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "GeTwOuZDMJ9Z",
    "outputId": "5b34b5ec-e6da-4c0b-87c7-8a085f07886c"
   },
   "outputs": [],
   "source": [
    "file_path = (\"preProcessedTokens.json\")\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_json(file_path)\n",
    "\n",
    "data = load_data(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAadbDOjUu6t"
   },
   "source": [
    "# Preprocess Your Training Data! üõ†Ô∏è\n",
    "Before we proceed with training our model, it's crucial to preprocess our training data to ensure it's in optimal shape. This involves dropping irrelevant columns, encoding categorical variables, and handling missing values using appropriate strategies.\n",
    "\n",
    "We'll meticulously clean and prepare our data, leaving no stone unturned to maximize the effectiveness of our machine learning model.\n",
    "\n",
    "Once our data is preprocessed and primed for training, we'll be one step closer to unveiling valuable insights and making informed decisions in the cryptocurrency market! üíº Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mP-qtJ8d0iOr"
   },
   "source": [
    "#Let's Split and Preprocess Your Data! üìä\n",
    "To ensure the reliability of our model, we'll first split our preprocessed data into training and testing sets. This allows us to train our model on one subset of data and evaluate its performance on another subset.\n",
    "\n",
    "Then, we'll construct a preprocessing pipeline to scale numeric features and encode categorical features. This ensures that our data is standardized and ready for our machine learning algorithm.\n",
    "\n",
    "Finally, we'll train an XGBoost classifier using the preprocessed data. XGBoost is a powerful algorithm known for its accuracy and efficiency, making it an ideal choice for our token risk assessment model.\n",
    "\n",
    "With our data split, preprocessed, and model trained, we're well-equipped to unlock valuable insights and make informed decisions in the dynamic cryptocurrency market! üí° Let's dive in and get started! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edyUbxTz0Mo3"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df = df.drop(['address', 'lastTradeUnixTime', 'mc'], axis=1)\n",
    "    X = df.drop('Risk', axis=1)\n",
    "    y = df['Risk'].map({'Danger': 1, 'Warning': 1, 'Good': 0}).astype(int)\n",
    "    return train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTnNrfaN0O45"
   },
   "outputs": [],
   "source": [
    "def build_preprocessor(X_train):\n",
    "    numeric_features = ['decimals', 'liquidity', 'v24hChangePercent', 'v24hUSD', 'Volatility', 'holders_count']\n",
    "    categorical_features = ['logoURI', 'name', 'symbol']\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "        )\n",
    "\n",
    "    return preprocessor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rpKPwaR_092C"
   },
   "source": [
    "#Train Your ML Model! üöÄ\n",
    "It's time to train your machine learning model and unleash its predictive prowess!\n",
    "\n",
    "By training your model, you'll enable it to learn from the data and identify patterns that help classify tokens based on their risk levels.\n",
    "\n",
    "With each iteration of training, your model becomes more adept at making accurate predictions, empowering you to navigate the Solana defi token market with confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ys6Dh4uH0_xO"
   },
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, preprocessor):\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAYAz4ZRF2WH"
   },
   "source": [
    "## Voila! Your Raw ML Model Is Ready! üéâ\n",
    "\n",
    "Congratulations on reaching this milestone! üöÄ With your raw ML model in hand, you now have the power to predict the risk associated with a token in the cryptocurrency market.\n",
    "\n",
    "Harnessing the capabilities of machine learning, you can now make informed investment decisions, mitigating risks and maximizing potential profits.\n",
    "\n",
    "But remember, this is just the beginning of your journey. Continuously refine and improve your model, stay updated with market trends, and never stop learning.\n",
    "\n",
    "Here's to your success in navigating the exciting world of cryptocurrency with confidence and precision! üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGIAzViHxZqe"
   },
   "source": [
    "#Let's Analyze Your Model's Performance! üòµ\n",
    "To evaluate the effectiveness of your classification model, we'll visualize its performance using a confusion matrix. This matrix provides insights into how well your model predicts the classes of a set of test data, where the true values are known. Here's what the confusion matrix looks like:\n",
    "\n",
    "\n",
    "```\n",
    "                   Predicted Class         \n",
    "            |   Positive   |   Negative   |\n",
    "Actual  ---------------------------------------\n",
    "Positive  |   TP         |       FP         |\n",
    "Negative  |   FN         |       TN         |\n",
    "\n",
    "```\n",
    "\n",
    "- TP (True Positive): Model correctly predicts positive class.\n",
    "- FP (False Positive): Model incorrectly predicts positive class.\n",
    "- FN (False Negative): Model incorrectly predicts negative class.\n",
    "- TN (True Negative): Model correctly predicts negative class.\n",
    "\n",
    "By examining these values, we'll gain valuable insights into how well your model performs in classifying tokens based on their risk levels. Let's dive in and analyze the results! üìä\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eB4Cy3Rz1Xy9"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_report_result = classification_report(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f'Model Accuracy: {accuracy}')\n",
    "    print('Classification Report:\\n', classification_report_result)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ul9t_tzG1dn3",
    "outputId": "25c6e3ff-b43f-47e7-e3b8-34c4a1db717b"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_path = 'preProcessedTokens.json'  # Update this path\n",
    "    df = load_data(file_path)\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(df)\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    preprocessor = build_preprocessor(X_train)\n",
    "    model = train_model(X_train, y_train, preprocessor)\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    # Save model and preprocessor\n",
    "    joblib.dump(model, \"predictModel.pkl\")\n",
    "    joblib.dump(preprocessor, \"mainPreprocessor.pkl\")\n",
    "\n",
    "    # Example for a single item prediction\n",
    "    single_item_corrected = {\n",
    "    \"decimals\": 6,\n",
    "    \"liquidity\": 62215.15524335994,\n",
    "    \"logoURI\": \"https://img.fotofolio.xyz/?url=https%3A%2F%2Fbafkreifhqihaiwyo4g2aogdu4qyfqftkxy3aq4xxbhoxdkbkufrobsnjwm.ipfs.nftstorage.link\",\n",
    "    \"name\": \"SBF\",\n",
    "    \"symbol\": \"SBF\",\n",
    "    \"v24hChangePercent\": -49.17844813082829,\n",
    "    \"v24hUSD\": 18220.724466666383,\n",
    "    \"Volatility\": 76.06539722778419,\n",
    "    \"holders_count\": 0\n",
    "}\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    single_item_df = pd.DataFrame(single_item_corrected, index=[0])\n",
    "    prediction = model.predict(single_item_df)  # Predict\n",
    "    print(f'Prediction for the single item: {prediction}')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAHdLc4jCcMY"
   },
   "source": [
    "# Save Your Model! üîΩ\n",
    "Now that your model is trained and ready to go, it's crucial to save it for seamless integration with your application.\n",
    "\n",
    "By saving your model, you can deploy it effortlessly and leverage its predictive power in real-world scenarios.\n",
    "\n",
    "Let's ensure your hard work doesn't go to waste by saving your model and paving the way for its successful implementation! üöÄ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "J9SUBve0KgVO"
   },
   "source": [
    "# üöÄ Data Fetching Expedition Begins!\n",
    "\n",
    "In this section, we embark on a thrilling journey to retrieve and whip into shape the data essential for predicting token behavior! üåü We're talking historical price data, token specifics, and holder count - the very backbone of our prediction! üìà\n",
    "\n",
    "We're not just fetching data; we're summoning it with flair! Utilizing the mighty Vybe Network as our data oracle, we dive into the digital depths to extract the insights that fuel our predictions! üí°\n",
    "\n",
    "But hold onto your hats, space cadets! To unlock this cosmic treasure trove of data, you'll need the legendary Vybe Networks API key. üóùÔ∏è Ready to blast off into the data cosmos? Let's do this! üöÄ\n",
    "\n",
    "First lets install and import the required libraries for this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0p3sbg_ss_dh"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "import dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets set up the environment variables.\n",
    "Just convert the .env.example file into a .env file and replace the ADD_YOUR_VYBE_API_KEY_HERE with yur actual vybe key. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "vybe_key = os.environ.get('VYBE_KEY')\n",
    "print(vybe_key) # verify your key has been properly loaded."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Ascending the Price History Peaks\n",
    "\n",
    "Our first stop: price history! With the get_token_price_history_with_retry function, we brave the digital currents to extract token quotes and OHLCV data. It's a journey fraught with challenges, but with each retry, we inch closer to the summit of knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxGJ04wQtBb1"
   },
   "outputs": [],
   "source": [
    "def get_token_price_history_with_retry(time_start, time_end, token_id, max_retries=3):\n",
    "    url = f\"https://api.vybenetwork.xyz/price/{token_id}/token-quote-ohlcv\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        'X-API-KEY': vybe_key\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"stride\": \"1 hour\",\n",
    "        \"time_end\": time_end,\n",
    "        \"time_start\": time_start\n",
    "    }\n",
    "\n",
    "    backoff_time = 1  # Initial backoff time in seconds\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "            if response.status_code == 200 or response.status_code == 204:\n",
    "                # Request was successful, you can handle the response here\n",
    "                return response.json()\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"Received 429 - Too Many Requests. Retrying in {backoff_time} seconds for {token_id}.\")\n",
    "                time.sleep(backoff_time)\n",
    "                backoff_time *= 2  # You can adjust this multiplier based on your needs\n",
    "            else:\n",
    "                # Handle the error\n",
    "                print(f\"Error: {response.status_code} - {response.text} for {token_id}. Retrying...\")\n",
    "                time.sleep(backoff_time)\n",
    "                backoff_time *= 2  # You can adjust this multiplier based on your needs\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle request exception\n",
    "            print(f\"Request exception: {e}, {token_id}\")\n",
    "            return f\"Request exception: {e}\"\n",
    "\n",
    "        # Increment backoff time for next retry\n",
    "        backoff_time *= 2\n",
    "\n",
    "        if attempt < max_retries - 1:\n",
    "            # Only sleep if there are more retries remaining\n",
    "            time.sleep(2)\n",
    "\n",
    "    print(f\"Maximum retries ({max_retries}) reached for {token_id}.\")\n",
    "    return None  # Or handle the failure in a different way as per your requirements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí• Unleashing Volatility Insights\n",
    "\n",
    "Next, we delve into volatility with the calculate_volatility function. Through the magic of standard deviation and daily returns, we gain insights into market fluctuations. It's not just about numbers; it's about understanding the ebb and flow of token dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_volatility(result):\n",
    "    DEFAULT_VOLATILITY_SCORE = None\n",
    "\n",
    "    if 'data' in result:\n",
    "        token_data = result['data']\n",
    "\n",
    "        # Extract relevant columns and create DataFrame\n",
    "        columns = ['timeBucketStart', 'open', 'high', 'low', 'close', 'count']\n",
    "        data = pd.DataFrame(token_data, columns=columns)\n",
    "\n",
    "        # Convert timeBucketStart to datetime and set it as index\n",
    "        data['timeBucketStart'] = pd.to_datetime(data['timeBucketStart'], unit='s')\n",
    "        data = data.set_index('timeBucketStart')\n",
    "\n",
    "        # Convert numerical columns to float\n",
    "        numerical_cols = ['open', 'high', 'low', 'close']\n",
    "        data[numerical_cols] = data[numerical_cols].astype(float)\n",
    "\n",
    "        # Calculate daily returns\n",
    "        data['Daily_Returns'] = data['close'].pct_change()\n",
    "\n",
    "        # Calculate volatility (standard deviation of daily returns)\n",
    "        volatility = np.std(data['Daily_Returns'])\n",
    "\n",
    "        # Normalize volatility to a scale of 1-100\n",
    "        min_volatility = np.min(data['Daily_Returns'])\n",
    "        max_volatility = np.max(data['Daily_Returns'])\n",
    "\n",
    "        # Check if the denominator is close to zero\n",
    "        if np.isclose(max_volatility, min_volatility):\n",
    "            print(\"Denominator is close to zero. Setting volatility score to default value.\")\n",
    "            return DEFAULT_VOLATILITY_SCORE\n",
    "        else:\n",
    "            # Perform the division only if the denominator is not close to zero\n",
    "            volatility_score = ((volatility - min_volatility) / (max_volatility - min_volatility)) * 100\n",
    "\n",
    "        return volatility_score\n",
    "\n",
    "    else:\n",
    "        return DEFAULT_VOLATILITY_SCORE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Embarking on a Token Details Expedition\n",
    "\n",
    "Our quest continues as we uncover token details with the get_token_details function. Armed with determination, we navigate through the digital landscape, overcoming obstacles to reveal the inner workings of tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_v24hChangePercent(token_data):\n",
    "    if 'data' in token_data and len(token_data['data']) >= 2:\n",
    "        first_close = float(token_data['data'][0]['close'])\n",
    "        last_close = float(token_data['data'][-1]['close'])\n",
    "        v24hChangePercent = ((last_close - first_close) / first_close) * 100\n",
    "        return v24hChangePercent\n",
    "    return None\n",
    "\n",
    "def get_token_details(token_id, max_retries=3):\n",
    "\n",
    "    url = f\"https://api.vybenetwork.xyz/token/{token_id}\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        'X-API-KEY': vybe_key  # Assuming vybe_key is defined elsewhere in your code\n",
    "    }\n",
    "\n",
    "    backoff_time = 1  # Initial backoff time in seconds\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200 or response.status_code == 204:\n",
    "                # Request was successful, you can handle the response here\n",
    "                return response.json()\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"Received 429 - Too Many Requests. Retrying in {backoff_time} seconds for {token_id}.\")\n",
    "                time.sleep(backoff_time)\n",
    "                backoff_time *= 2  # You can adjust this multiplier based on your needs\n",
    "            else:\n",
    "                # Handle the error\n",
    "                print(f\"Error: {response.status_code} - {response.text} for {token_id}. Retrying...\")\n",
    "                time.sleep(backoff_time)\n",
    "                backoff_time *= 2  # You can adjust this multiplier based on your needs\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle request exception\n",
    "            print(f\"Request exception: {e}, {token_id}\")\n",
    "            return f\"Request exception: {e}\"\n",
    "\n",
    "        # Increment backoff time for next retry\n",
    "        backoff_time *= 2\n",
    "\n",
    "        if attempt < max_retries - 1:\n",
    "            # Only sleep if there are more retries remaining\n",
    "            time.sleep(2)\n",
    "\n",
    "    print(f\"Maximum retries ({max_retries}) reached for {token_id}.\")\n",
    "    return None  # Or handle the failure in a different way as per your requirements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåä Surfing the Waves of Liquidity\n",
    "\n",
    "Dive into the depths of liquidity with the calculate_liquidity function. As we ride the waves of market cap and token volume, we gain a deeper understanding of liquidity's role in the token ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_liquidity(token_data):\n",
    "    if 'marketCap' in token_data and 'tokenAmountVolume' in token_data:\n",
    "        market_cap = token_data['marketCap']\n",
    "        token_volume = token_data['tokenAmountVolume']\n",
    "        if token_volume is not None and token_volume > 0:\n",
    "            liquidity = market_cap / token_volume\n",
    "            return liquidity\n",
    "    return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë• Exploring Holder Count Mysteries\n",
    "\n",
    "Last but not least, we explore holder counts with the get_number_of_holders function. It's a journey through API calls and response codes, uncovering the strength of token communities one holder at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_holders(token_id, interval='day'):\n",
    "\n",
    "    url = f\"https://api.vybenetwork.xyz/token/{token_id}/holders-ts\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        'X-API-KEY': vybe_key\n",
    "    }\n",
    "    params = {\n",
    "        \"interval\": interval,\n",
    "        \"time_end\": 'null',\n",
    "        \"time_start\": 'null'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][-1]['nHolders']\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response}\")\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Embark on a Data Fetch Odyssey\n",
    "\n",
    "Join us on a data-fetching odyssey with the fetchDataFunc function. From token details to price history, from volatility scores to holder counts, we gather the essence of tokens, shaping them into a beacon of knowledge to guide us through the digital cosmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetchDataFunc(token_id):\n",
    "    token_data = get_token_details(token_id)\n",
    "    time_start = int(time.time()) - (24 * 60 * 60)\n",
    "    time_end = int(time.time())\n",
    "    token_OHLCV_data =  get_token_price_history_with_retry(time_start, time_end, token_id)\n",
    "    v24hChangePercent = calculate_v24hChangePercent(token_OHLCV_data)\n",
    "    liquidity = calculate_liquidity(token_data)\n",
    "    volatility_score = calculate_volatility(token_OHLCV_data)\n",
    "    holder_count =  get_number_of_holders(token_id)\n",
    "    v24hUSD = 0\n",
    "    if token_data['usdValueVolume'] is not None:\n",
    "        v24hUSD = token_data['usdValueVolume']\n",
    "    input_data = {\n",
    "        \"decimals\": token_data['decimal'],\n",
    "        \"liquidity\":liquidity,\n",
    "        \"logoURI\":1,\n",
    "        \"name\": 1,\n",
    "        \"symbol\": 1,\n",
    "        \"v24hChangePercent\": v24hChangePercent,\n",
    "        \"v24hUSD\": v24hUSD,\n",
    "        \"Volatility\": volatility_score,\n",
    "        \"holders_count\": holder_count\n",
    "        }\n",
    "    return input_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kNQSeuebCf9t"
   },
   "source": [
    "# üîÆ Predict Function Unveiled\n",
    "\n",
    "In this segment, we demystify the process of prediction. üåü Here, we simply showcase how our saved model, snugly nestled within a pkl file, comes to life. üì¶ With a straightforward invocation of the predict method, we harness the power of our input data to yield a binary classification. üí´\n",
    "\n",
    "It's a peek behind the curtain, revealing the simplicity that underpins the magic of prediction. So relax and enjoy the gentle unveiling of this fundamental step in our data journey! üîç‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riQcATIqtLha"
   },
   "outputs": [],
   "source": [
    "async def predict_token_risk(input_data):\n",
    "    \"\"\"Predicts the risk of a token based on the input parameters.\n",
    "\n",
    "    Args:\n",
    "        input_data (dict): Input data containing token address.\n",
    "\n",
    "    Returns:\n",
    "        int: 0 is safe and 1 is dangerous\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = joblib.load(\"predictModel.pkl\")\n",
    "        token_id = input_data['token_address']\n",
    "        as_dict = await fetchDataFunc(token_id)\n",
    "        single_item_df = pd.DataFrame(as_dict, index=[0])\n",
    "        prediction = model.predict(single_item_df)  # Predict\n",
    "        single_prediction = prediction[0]  # Extract single element\n",
    "        return int(single_prediction)  # Convert prediction to int\n",
    "    except Exception as e:\n",
    "        # Handle exceptions appropriately\n",
    "        raise RuntimeError(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GIGqg2IhGfxq"
   },
   "source": [
    "# üîç Token Inquiry: A Data Adventure\n",
    "\n",
    "Welcome to the heart of our data journey! üåü Here, in this interactive segment, we invite you to embark on an exploration of token safety. üíº\n",
    "\n",
    "Simply run the following cell, and it will beckon you to provide a token address. Once entered, our trusty data-fetching code springs into action, gathering all the relevant data features needed to assess the safety of your chosen token. üìä\n",
    "\n",
    "With bated breath, you'll soon discover whether your token is deemed safe or harbors any danger. It's a journey of discovery and insight, unfolding with each inquiry. So, without further ado, let's dive in and explore the safety of tokens together! üöÄüîç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QPOp5CizttPP",
    "outputId": "3b788667-1089-4ccb-bc56-7bcf2f61b6c7"
   },
   "outputs": [],
   "source": [
    "async def main():\n",
    "    while True:\n",
    "        # Get user input for token address\n",
    "        token_address = input(\"Enter token address (or 'q' to exit): \").strip()\n",
    "\n",
    "        # Check if user wants to quit\n",
    "        if token_address.lower() == 'q':\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Create input data dictionary\n",
    "        input_data = {\"token_address\": token_address}\n",
    "\n",
    "        # Call predict_token_risk_async function\n",
    "        try:\n",
    "            risk_level = await predict_token_risk(input_data)\n",
    "            if risk_level == 0:\n",
    "              print(\"Risk Level: Safe: \", risk_level)\n",
    "            elif risk_level == 1:\n",
    "              print(\"Risk Level: Danger: \", risk_level)\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred:\", e)\n",
    "\n",
    "# Run the async main function in the event loop\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
